{"pageType": "article", "pathParams": {"slug": "llvm-protothreads"}, "locals": {"article": {"body": "<p>The peak size of the stack for a given thread of execution in a computer\nprogram is impossible to predict in general, and even when it can be\npredicted (in the absense of recursion and conditional allocation) the\npeak size can be a pretty pessimistic estimate of the actual stack\nrequirements.</p><p>As a consequence, threading support in modern operating systems, as well as\napplication-level threads as implemented by some languages such as Go, tends\nto allocate each thread a pretty large stack. This is a good tradeoff on a\nmachine with a virtual memory manager and plenty of address space per process,\nbut is unreasonable on a memory-constrained system like a microcontroller.</p><p><em>Protothreads</em>, or \"stackless threads\", are a tool for providing multiple\nthreads of execution without allocating a separate stack for each. This feat\nis achieved by placing several constraints on the code within the\nthread: blocking is only permitted in the top-level function of the thread,\nthe top-level function's local variables cannot be allocated on the stack\nnor persisted in registers, and these \"threads\" can only yield to others\nat predefined points in the program (co-operative scheduling).</p><p>With these to constraints imposed, it is possible to simulate blocking by\nreturning control to the caller (a basic scheduler, presumably) after retaining\na record of where in the function execution should resume on the next call.\nThis technique has most commonly been implemented in C via preprocessor macros,\nsuch as in\n<a href=\"http://dunkels.com/adam/pt/\" target=\"_blank\">Adam Dunkels' Protothreads Library</a>. Here is\na raw, macro-free example using the GCC computed goto extension, which is\nfully general:</p><p>Notice that the position is retained in a <tt>static</tt> variable to ensure that\nits value persists across calls. Any other local variables must also be\ndeclared as <tt>static</tt> for this technique to work.</p><p>Although clever use of macros allows the details of this technique to be\nsomewhat hidden in C, the fact that it is being implemented <em>in spite of</em>\nthe language prevents the result from being fully natural.</p><p>However, a hypothetical language that has protothreads as a built-in feature\ncan in principle hide the plumbing in a much more robust way, by providing\nnative language constructs for yielding and by automatically hoisting all of\nthe local variables into global variables.</p><p>Such a language would still have to generate something resembling the above\nC code in its backend, however. In that vein, I set about seeing what this\ntechnique could look like implemented in LLVM assembly, which is a\nsomewhat-popular choice of intermediate representation between a language's\nparser and its code generation backend.</p><p>Let's dive in.</p><section id=\"data-structures\"><h3>Data Structures</h3><p>To implement a basic cooperative scheduler we need to start with a data\nstructure representing a thread, and then along with that a queue of threads\nthat are ready to run. For simplicity's sake, I combined these two concepts\ntogether to make a doubly-linked list of runnable threads, whose members\nare of type <tt>%cont</tt>, which is short for \"continuation\":</p><p>This design is slightly more sophisticated than the C example above since it\nallows for multiple instances of the same thread code to be active at once,\neach with its own \"context\". The context here is typed as <tt>i8*</tt> but that\ntype is just a placeholder for a pointer to some data structure that's\nconstructed from the local variables of the implementation function.</p><p>We can wrap around that a straightforward \"queue\" concept:</p><p>In order to simplify management of the linked list, at the expense of some\nnon-obvious trickery, the pointer to the queue instance itself -- after a\nbitcast to type <tt>%cont</tt> -- is used as the \"end of list\" sentinel. By placing\nthe queue structure members in \"reverse order\", the list manipulation code\ncan simply treat the queue as a list item, automatically updating the\nlast/first pointers when items are added and removed from the edges of the\nlist.</p><p>By embedding the linked list pointers inside the continuation structure we\ncreate the constraint that each continuation can be in only one queue at a\ntime, which suffices for this test, since there's only one queue.</p><p>Finally, we create the type for the scheduler itself and the singleton global\ninstance of it:</p><p>As promised, we <tt>bitcast</tt> the pointer to the queue so LLVM will accept it\nas a pointer to a <tt>%cont</tt>, and assign it as both the head and tail of the\nlist, creating an empty list.</p></section><section id=\"queue-management\"><h3>Queue Management</h3><p>There are only three operations supported on the ready queue: appending an\nitem to it, signalling that it has become ready, and removing an item from\nit, signalling that it has become blocked.</p><p>These are just normal linked list operations, although they are a a little\nmore verbose than usual when written in LLVM IR, due to the need to explicitly\ndereference all of the structs and pointers:</p><p>Our strange handling of the sentinel values pays off here, since the code\nfor managing the list \"accidentally\" updates the head/tail pointers in the\nqueue structure when the item being manipulated happens to be at one of the\nends of the list, just by dereferencing the sentinel pointer that refers\nto the queue.</p></section><section id=\"the-scheduler-function\"><h3>The Scheduler Function</h3><p>Next up is the scheduler itself. The flow here is pretty simple.\nFirst we locate the first item in the queue, then we remove it from the queue,\nand then we execute the given implementation function with the state data as\nparameters. Once the queue is empty (signalled by the first item being the\nqueue itself), the scheduler returns:</p></section><section id=\"thread-implementations\"><h3>Thread Implementations</h3><p>Now we just need some thread implementation functions to test with. To keep\nthings simple, our thread functions will just loop a certain number of times,\nyielding to the scheduler after each iteration, before finally exiting.\nThe final program has two threads, but since their implementations are largely\nidentical I'll focus on only one here.</p><p>First we need to allocate the global variables that will represent the\nthread's state. In a real compiler the thread's context would be a structure\ntype, but to keep things simple in this hand-written LLVM assembly I just\nused a single integer:</p><p>The <tt>bitcast</tt> to <tt>i8*</tt> is of course unnecessary since the context type\nis already of this type, but it's included to show that in a real\nimplementation such a cast would be required.</p><p>That just leaves the thread's implementation function:</p><p>The <tt>indirectbr</tt> at the beginning of the function is exactly equivalent\nto the computed <tt>goto</tt> we used in the original C example, but in this\ncase the target is passed as a parameter since static local variables are\nnot a concept in LLVM.</p><p>The code implementing the yield to the scheduler is worth isolating to better\nsee how it works:</p><p>We use the <tt>@cont_ready</tt> function from earlier to place the thread back into\nthe ready queue (we have no events to block on here, so we're just giving other\nthreads an opportunity to run). The <tt>blockaddress</tt> expression serves the\nsame purpose as the <tt>&amp;&amp;label</tt> syntax in GCC's computed goto C extension.</p></section><section id=\"testing-it-out\"><h3>Testing It Out</h3><p>Finally, we just need a little entry point function to kick things off:</p><p>Again we need to mark the threads as ready, but this time we make the\ncontinuation refer to the <tt>begin</tt> block in each function.</p><p>Once translated to native assembly, linked, and executed, the result looks\nsomething like this:</p><pre class='terminal'>Hello\nSched Loop\nSched Done\nSched Loop\nSched Do\nThread 1 Start\nThread 1 Loop\nSched Loop\nSched Do\nThread 2 Start\nThread 2 Loop\nSched Loop\nSched Do\nThread 1 Loop\nSched Loop\nSched Do\nThread 2 Loop\nSched Loop\nSched Do\nThread 1 Loop\nThread 1 Exit\nSched Loop\nSched Do\nThread 2 Loop\nSched Loop\nSched Do\nThread 2 Loop\nThread 2 Exit\nSched Loop\nSched Done</pre><p>This output (produced by printing some strings that weren't included in the\nabove snippets, but are in the full program linked below) shows how each\nrun of the scheduler runs a section of code from one of the threads, with\neach of them starting up, looping several times, and then exiting. The\nsecond thread runs two more iterations than the first before it exits.</p></section><section id=\"conclusion\"><h3>Conclusion</h3><p>Hand-writing LLVM assembly gets a bit verbose and tedious at times, but this\nexample shows that a compiler using LLVM as a backend can implement a basic\nprotothread scheduler with only a few modifications to the code generator:</p><ul><li><p>Arrange for thread functions to load/store their local variables through the\ncontext structure rather than through pointers allocated with <tt>alloca</tt>.</p></li><li><p>Ensure that the generated code does not depend on the values in named\nregisters persisting across a blocking call.</p></li><li><p>Insert thread-scheduling calls and extra <tt>ret</tt> statements into the program\nat points where a thread will yield.</p></li></ul><p>The first of these points does, however, impose an optimization penalty on\nthe generated function: usually simple local variables created with <tt>alloca</tt>\nare turned into simple registers by the <tt>mem2reg</tt> optimization pass, but\nthis optimization is not possible when the local variables are effectively\nglobal. The resulting code will therefore have many more <tt>load</tt> and <tt>store</tt>\noperations than normal, though other optimization passes may help somewhat.</p><p><a href=\"https://gist.github.com/apparentlymart/5c62e4a794c52939986a\" target=\"_blank\">The full program, and a Makefile to build it</a>,\nare available for those who would like to study it further or build something\nfrom it. It's released into the public domain in the hope that it will be\nuseful, but with no warranty of quality or fitness for purpose: it's just a\nprototype.</p><p>I intend to apply a technique similar to this in my work-in-progress\nprogramming language <a href=\"http://martin.atkins.me.uk/alamatic/\" target=\"_blank\">Alamatic</a>, which\nattempts to bring the power of modern abstractions to the\nlimited environment of microcontrollers. It is my hope that protothreads will\nallow for a more natural programming model when dealing with asynchronous\noperations, as well as affording good composability between device driver\ncode and application code, while avoiding the need to juggle multiple stacks in\na small address space.</p></section>", "mainImageUrl": "/llvm-protothreads/None", "title": "Protothreads in LLVM IR", "url": "/llvm-protothreads/", "summary": "Implementing stackless threads within LLVM", "date": "2014-09-13", "type": "Programming Language Design"}}, "path": "/:slug/"}